"""
Módulo de Descoberta de Endpoints
- Web crawling
- Diretory/file bruteforce
- API endpoint discovery
- Robots.txt e sitemap.xml parsing
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
import concurrent.futures
from typing import Set, List, Dict
import re


class EndpointDiscovery:
    def __init__(self, base_url: str, timeout: int = 5, max_depth: int = 3):
        self.base_url = base_url.rstrip('/')
        self.timeout = timeout
        self.max_depth = max_depth
        self.visited_urls = set()
        self.discovered_endpoints = set()
        self.forms = []
        self.parameters = {}
        self.session = requests.Session()
        self.session.verify = False
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def crawl(self, start_url: str = None, depth: int = 0) -> Set[str]:
        """
        Crawl website recursivamente para descobrir endpoints
        """
        if start_url is None:
            start_url = self.base_url

        if depth > self.max_depth or start_url in self.visited_urls:
            return self.discovered_endpoints

        print(f"[*] Crawling: {start_url} (depth: {depth})")
        self.visited_urls.add(start_url)

        try:
            response = self.session.get(start_url, timeout=self.timeout)
            self.discovered_endpoints.add(start_url)

            if 'text/html' in response.headers.get('Content-Type', ''):
                soup = BeautifulSoup(response.text, 'html.parser')

                # Extrai links
                for link in soup.find_all('a', href=True):
                    url = urljoin(start_url, link['href'])
                    if self._is_valid_url(url) and url not in self.visited_urls:
                        self.crawl(url, depth + 1)

                # Extrai formulários
                forms = self._extract_forms(soup, start_url)
                self.forms.extend(forms)

                # Extrai parâmetros de scripts
                self._extract_parameters_from_scripts(soup)

        except Exception as e:
            print(f"[-] Erro ao crawl {start_url}: {e}")

        return self.discovered_endpoints

    def _is_valid_url(self, url: str) -> bool:
        """Verifica se URL pertence ao domínio alvo"""
        parsed_base = urlparse(self.base_url)
        parsed_url = urlparse(url)

        # Ignora URLs externas, anchors, mailto, tel, etc
        if parsed_url.scheme not in ['http', 'https']:
            return False

        if parsed_url.netloc and parsed_url.netloc != parsed_base.netloc:
            return False

        # Ignora arquivos estáticos comuns
        static_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.css', '.js',
                           '.pdf', '.zip', '.tar', '.gz', '.svg', '.ico']
        if any(url.lower().endswith(ext) for ext in static_extensions):
            return False

        return True

    def _extract_forms(self, soup: BeautifulSoup, url: str) -> List[Dict]:
        """Extrai todos os formulários da página"""
        forms = []

        for form in soup.find_all('form'):
            form_details = {
                'url': url,
                'action': urljoin(url, form.get('action', '')),
                'method': form.get('method', 'get').lower(),
                'inputs': []
            }

            # Extrai inputs
            for input_tag in form.find_all(['input', 'textarea', 'select']):
                input_details = {
                    'type': input_tag.get('type', 'text'),
                    'name': input_tag.get('name'),
                    'value': input_tag.get('value', '')
                }
                form_details['inputs'].append(input_details)

            forms.append(form_details)
            print(f"[+] Formulário encontrado: {form_details['action']} ({form_details['method']})")

        return forms

    def _extract_parameters_from_scripts(self, soup: BeautifulSoup):
        """Extrai parâmetros potenciais de scripts JavaScript"""
        scripts = soup.find_all('script')

        for script in scripts:
            if script.string:
                # Procura por padrões de API endpoints
                api_patterns = [
                    r'["\']/(api/[^"\']+)["\']',
                    r'fetch\(["\']([^"\']+)["\']',
                    r'axios\.[get|post]+\(["\']([^"\']+)["\']',
                    r'\.ajax\({[^}]*url:\s*["\']([^"\']+)["\']',
                ]

                for pattern in api_patterns:
                    matches = re.findall(pattern, script.string)
                    for match in matches:
                        endpoint = urljoin(self.base_url, match)
                        if self._is_valid_url(endpoint):
                            self.discovered_endpoints.add(endpoint)
                            print(f"[+] API endpoint encontrado: {endpoint}")

    def directory_bruteforce(self, wordlist: List[str] = None) -> Set[str]:
        """
        Brute force de diretórios e arquivos comuns
        """
        print(f"[*] Iniciando directory bruteforce em {self.base_url}...")

        if wordlist is None:
            # Wordlist básica de diretórios e arquivos comuns
            wordlist = [
                'admin', 'administrator', 'login', 'dashboard', 'panel',
                'api', 'v1', 'v2', 'test', 'dev', 'staging',
                'backup', 'old', 'tmp', 'temp', 'uploads',
                'images', 'img', 'static', 'assets', 'files',
                'config', 'conf', 'configuration', 'settings',
                'user', 'users', 'account', 'profile', 'auth',
                'wp-admin', 'phpmyadmin', 'mysql', 'database',
                '.git', '.env', '.htaccess', '.config',
                'readme.md', 'README.md', 'changelog.txt',
                'backup.sql', 'dump.sql', 'config.php',
                'web.config', 'app.config', 'settings.php'
            ]

        found = set()

        def check_path(path):
            url = f"{self.base_url}/{path}"
            try:
                response = self.session.get(url, timeout=self.timeout,
                                           allow_redirects=False)

                # 200 = encontrado, 403 = forbidden (mas existe)
                if response.status_code in [200, 403]:
                    print(f"[+] Encontrado [{response.status_code}]: {url}")
                    return url
                elif response.status_code == 301 or response.status_code == 302:
                    print(f"[+] Redirect [{response.status_code}]: {url} -> {response.headers.get('Location')}")
                    return url
            except:
                pass
            return None

        with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
            results = executor.map(check_path, wordlist)
            found = {r for r in results if r}

        self.discovered_endpoints.update(found)
        print(f"[+] Directory bruteforce concluído: {len(found)} paths encontrados")

        return found

    def check_common_files(self) -> Dict[str, bool]:
        """
        Verifica arquivos comuns importantes
        """
        print(f"[*] Verificando arquivos comuns...")

        common_files = {
            'robots.txt': '/robots.txt',
            'sitemap.xml': '/sitemap.xml',
            'security.txt': '/.well-known/security.txt',
            '.git/config': '/.git/config',
            '.env': '/.env',
            'package.json': '/package.json',
            'composer.json': '/composer.json',
            'README.md': '/README.md',
            'phpinfo.php': '/phpinfo.php',
            'info.php': '/info.php',
            'test.php': '/test.php',
            'admin': '/admin',
            'administrator': '/administrator',
        }

        results = {}

        for name, path in common_files.items():
            url = f"{self.base_url}{path}"
            try:
                response = self.session.get(url, timeout=self.timeout)
                if response.status_code == 200:
                    print(f"[+] Arquivo encontrado: {name} ({url})")
                    results[name] = True
                    self.discovered_endpoints.add(url)

                    # Parse robots.txt e sitemap.xml
                    if name == 'robots.txt':
                        self._parse_robots(response.text)
                    elif name == 'sitemap.xml':
                        self._parse_sitemap(response.text)
                else:
                    results[name] = False
            except:
                results[name] = False

        return results

    def _parse_robots(self, content: str):
        """Parse robots.txt para encontrar endpoints"""
        print("[*] Parsing robots.txt...")

        for line in content.split('\n'):
            line = line.strip()
            if line.startswith('Disallow:') or line.startswith('Allow:'):
                path = line.split(':', 1)[1].strip()
                if path and path != '/':
                    url = urljoin(self.base_url, path)
                    self.discovered_endpoints.add(url)
                    print(f"[+] Endpoint do robots.txt: {url}")

    def _parse_sitemap(self, content: str):
        """Parse sitemap.xml para encontrar URLs"""
        print("[*] Parsing sitemap.xml...")

        try:
            soup = BeautifulSoup(content, 'xml')
            urls = soup.find_all('loc')

            for url_tag in urls:
                url = url_tag.text
                if self._is_valid_url(url):
                    self.discovered_endpoints.add(url)
                    print(f"[+] URL do sitemap: {url}")
        except Exception as e:
            print(f"[-] Erro ao parse sitemap: {e}")

    def detect_api_endpoints(self) -> List[str]:
        """
        Detecta endpoints de API comuns
        """
        print("[*] Procurando por endpoints de API...")

        api_patterns = [
            '/api', '/api/v1', '/api/v2', '/api/v3',
            '/rest', '/rest/v1', '/rest/v2',
            '/graphql', '/gql',
            '/api/users', '/api/user', '/api/auth',
            '/api/login', '/api/register', '/api/products',
            '/v1/users', '/v1/auth', '/v2/users', '/v2/auth',
            '/swagger', '/swagger.json', '/swagger-ui',
            '/api-docs', '/docs', '/documentation',
            '/openapi.json', '/openapi.yaml'
        ]

        found = []

        for pattern in api_patterns:
            url = f"{self.base_url}{pattern}"
            try:
                response = self.session.get(url, timeout=self.timeout)
                if response.status_code in [200, 401, 403]:
                    print(f"[+] API endpoint encontrado: {url} [{response.status_code}]")
                    found.append(url)
                    self.discovered_endpoints.add(url)
            except:
                pass

        return found

    def get_full_report(self) -> Dict:
        """Retorna relatório completo da descoberta"""
        return {
            'base_url': self.base_url,
            'discovered_endpoints': list(self.discovered_endpoints),
            'total_endpoints': len(self.discovered_endpoints),
            'forms': self.forms,
            'visited_urls': list(self.visited_urls)
        }
